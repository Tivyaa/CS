import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

# Download NLTK resources only once
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('averaged_perceptron_tagger_eng', quiet=True)

text = """Deep learning is a subset of machine learning, which is a branch of artificial intelligence.
It uses neural networks with multiple layers to automatically extract features from data."""

sentences = sent_tokenize(text)
print("Sentence Tokenization:\n", sentences)

words = word_tokenize(text)
print("\nWord Tokenization:\n", words)

pos_tags = nltk.pos_tag(words)
print("\nPart of Speech Tagging:\n", pos_tags)

words_lower = [word.lower() for word in words if word.isalpha()]
print("\nAfter Lowercasing and Punctuation Removal:\n", words_lower)
